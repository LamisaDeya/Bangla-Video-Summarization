!pip install transformers
!pip install datasets
!pip install torch
!pip install openpyxl  # Required for reading Excel files
!pip install tqdm  # Progress bar
!pip install evaluate
!pip install rouge_score                                                                                                                                                                      import os
import pandas as pd
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from tqdm import tqdm  # Ensure you import tqdm for progress bars
import evaluate
import torch
import gc
from transformers import TrainerCallback

------

# Disable wandb logging
os.environ["WANDB_DISABLED"] = "true"

# Mount Google Drive (if necessary)
#drive.mount('/content/drive')

# Step 1: Load the dataset
df = pd.read_excel("/kaggle/input/ket-2k/Ket.xlsx")  # Adjust path

# Ensure the dataset has two columns: 'text' and 'summary'
df = df.rename(columns={df.columns[0]: "text", df.columns[1]: "summary"})
df = df.dropna()
#df=df.head(10)

---------------

# Step 2: Split the dataset into train, validation, and test sets (60% train, 20% val, 20% test)
train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42)  # 60% for training, 40% remaining
val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42)  # Split the remaining 40% into 50% val and 50% test

# Convert pandas DataFrames into Hugging Face Dataset objects
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
test_dataset = Dataset.from_pandas(test_df)

# Combine datasets into a DatasetDict for Hugging Face Trainer
dataset_dict = DatasetDict({"train": train_dataset, "val": val_dataset, "test": test_dataset})

-----------

# Step 3: Load the tokenizer and model (using Bangla T5 for summarization)
model_name = "csebuetnlp/banglat5"  # Ensure this is a seq2seq model

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Clear RAM before loading a new model or changing configurations
def clear_ram():
    """Clear up the GPU memory and release model from RAM."""
    print("Clearing RAM...")
    torch.cuda.empty_cache()  # Clear GPU cache
    gc.collect()  # Run garbage collection to release other resources
    print("RAM cleared.")

clear_ram()

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
----------

# Move model to the selected device (GPU or CPU)
model.to(device)

# Tokenization function for summarization
def preprocess_function(examples):
    # Tokenize the input text
    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(examples['summary'], max_length=512, truncation=True, padding="max_length")

    # Add labels to inputs
    inputs["labels"] = labels["input_ids"]

    # Convert lists to tensors and move them to the device
    # Move inputs to the device (GPU or CPU) for both input and label tensors
    inputs = {key: torch.tensor(value).to(device) for key, value in inputs.items()}

    return inputs

# Apply tokenization to the train and validation datasets
tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)

----------------

# Step 4: Set up training arguments (adjust batch size here)
training_args = Seq2SeqTrainingArguments(
    output_dir="/kaggle/working/results",  # Change output directory to Kaggle's working space
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,  # Adjusted batch size to 4
    per_device_eval_batch_size=4,   # Adjusted batch size to 4
    num_train_epochs=10,
    weight_decay=0.01,
    logging_dir='/kaggle/working/logs',  # Change logging directory to Kaggle's working space
    logging_steps=10,
    predict_with_generate=True,
    run_name="bangla_summarization_run",  # Optional: set a different run name
    report_to="none"  # Disable WANDB and other integrations
)
----------------

# Step 5: Initialize and train with the custom trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['val'],
    tokenizer=tokenizer
)

# Iterate over all the shards and train on each shard
for shard_idx, shard in enumerate(sharded_train_datasets):
    print(f"Training on shard {shard_idx + 1}/{len(sharded_train_datasets)}")
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=shard,  # Use the current shard
        eval_dataset=tokenized_datasets['val'],
        tokenizer=tokenizer,
    )

# Step 6: Train the model
trainer.train()



# Step 7: Save the fine-tuned model and tokenizer at the end
def save_model(model, output_dir):
    for param in model.parameters():
        if not param.is_contiguous():
            param.data = param.data.contiguous()
    model.save_pretrained(output_dir)

save_model(model, "/kaggle/working/fine_tuned_bangla_t5")  # Save the final model in Kaggle's working directory
tokenizer.save_pretrained("/kaggle/working/fine_tuned_bangla_t5")  # Save the tokenizer as well               
inputs = tokenizer(text, max_length=512, truncation=True, return_tensors="pt", padding="max_length")

    # Move input tensors to the same device as the model
    inputs = {key: value.to(device) for key, value in inputs.items()}

    # Generate summary
    summary_ids = model.generate(
        inputs['input_ids'],
        max_length=512,
        num_beams=3,
        no_repeat_ngram_size=2,
        early_stopping=True
    )

    # Decode and return the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

---------------

# Step 12: Generate summaries for the validation dataset with a progress bar
# Use tqdm to show the progress during inference
test_df['generated_summary'] = [
    generate_summary(text, model, tokenizer, device)
    for text in tqdm(test_df['text'], desc="Generating summaries", unit="text")
]

# Step 13: Load the ROUGE metric from the 'evaluate' library
rouge = evaluate.load('rouge')

# Step 14: Calculate the final ROUGE score for all predictions and references
references = test_df['summary'].tolist()
predictions = test_df['generated_summary'].tolist()

# Compute the final ROUGE score across the whole test set
final_rouge_score = rouge.compute(predictions=predictions, references=references)

# Print the final ROUGE scores
print(f"Final ROUGE Score: {final_rouge_score}")

# Save the output to a new Excel file with generated summaries and final ROUGE scores
test_df.to_excel("/kaggle/working/test_data_with_summaries_and_rouge.xlsx", index=False)                                                        # Print the result with generated summaries
print(test_df[['text', 'summary', 'generated_summary']])